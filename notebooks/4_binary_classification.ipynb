{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Binary classification\n",
    "\n",
    "What follows is the code implementing the linear classification example.\n",
    "\n",
    "It does not converge -- and neither does any other unmodified implementation I have found on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "from utils.sugar import *\n",
    "\n",
    "dataset = (((1.2, 0.7), +1.0), ((-0.3, 0.5), -1.0), ((-3.0, -1.0), +1.0),\n",
    "           ((0.1, 1.0), -1.0), ((3.0, 1.1), -1.0), ((2.1, -3.0), +1.0))\n",
    "\n",
    "a, b, c = param(1, -2, -1)  # initial solution\n",
    "x, y, label = const(0, 0, 0)  # not affected by backprop\n",
    "f = minimum(1, label * (a * x + b * y + c))\n",
    "regularization = 1.0\n",
    "\n",
    "for iteration in range(500):\n",
    "\n",
    "    if iteration % 50 == 0 or (iteration % 10 == 0 and iteration < 40):\n",
    "        correct = sum(f.compute() > 0 for (x.val, y.val), label.val in dataset)\n",
    "        print 'Accuracy at iteration {}: {:.1f} [{:.2f} {:.2f} {:.2f}]'.format(\n",
    "            iteration, (100.0 * correct) / len(dataset), a.val, b.val, c.val)\n",
    "\n",
    "    (x.val, y.val), label.val = choice(dataset)\n",
    "    f.compute()\n",
    "    f.backprop()\n",
    "\n",
    "    a.grad -= a.val * regularization\n",
    "    b.grad -= b.val * regularization\n",
    "\n",
    "    f.update_parameters(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that the `regularization` applied to parameters `a` and `b` dominates the parameter update.\n",
    "\n",
    "Making `regularization` very small or 0 allows for convergence, although at an estremely slow pace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = param(1, -2, -1)  # initial solution\n",
    "x, y, label = const(0, 0, 0)  # not affected by backprop\n",
    "f = minimum(1, label * (a * x + b * y + c))\n",
    "\n",
    "for iteration in range(35001):\n",
    "\n",
    "    if iteration % 2500 == 0 or (iteration % 10 == 0 and iteration < 40):\n",
    "        correct = sum(f.compute() > 0 for (x.val, y.val), label.val in dataset)\n",
    "        print 'Accuracy at iteration {}: {:.1f} [{:.2f} {:.2f} {:.2f}]'.format(\n",
    "            iteration, (100.0 * correct) / len(dataset), a.val, b.val, c.val)\n",
    "\n",
    "    (x.val, y.val), label.val = choice(dataset)\n",
    "    f.compute()\n",
    "    f.backprop(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a single sample to compute gradients produces an update direction nearly orthogonal to that of the closest optimal solution.\n",
    "\n",
    "Using a batch size larger than 1 or the full dataset does not help, since the projection of Jacobian in the direction of the closest optimal solution is regardless very small (proportional to the amount of gradients computed).\n",
    "\n",
    "Adding a global multiplier to the `f` function results in a 3X speed-up despite adding a degree of freedom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c, m = param(1, -2, -1, 1)  # initial solution\n",
    "x, y, label = const(0, 0, 0)  # not affected by backprop\n",
    "f = minimum(1, label * m * (a * x + b * y + c))\n",
    "\n",
    "for iteration in range(12001):\n",
    "\n",
    "    if iteration % 500 == 0 or (iteration % 10 == 0 and iteration < 40):\n",
    "        correct = sum(f.compute() > 0 for (x.val, y.val), label.val in dataset)\n",
    "        print 'Accuracy at iteration {}: {:.1f} [{:.2f} * ({:.2f}, {:.2f}, {:.2f})]'.format(\n",
    "            iteration, (100.0 * correct) / len(dataset), m.val, a.val, b.val, c.val)\n",
    "\n",
    "    (x.val, y.val), label.val = choice(dataset)\n",
    "    f.compute()\n",
    "    f.backprop(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asking for the norm of `(a, b, c)` to be 1 again creates competition between the two objectives, making the constraint either counterproductive or ineffective. Making it converge requires careful balancing of gradients propagated and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 0: 66.7 [1.00 -2.00 -1.00]\nAccuracy at iteration 10: 66.7 [0.76 -2.00 -0.70]\nAccuracy at iteration 20: 83.3 [0.22 -2.09 -0.10]\nAccuracy at iteration 30: 83.3 [0.16 -2.06 -0.00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 2500: 83.3 [0.90 -4.97 1.90]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 5000: 83.3 [1.29 -7.29 2.90]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 7500: 83.3 [1.86 -9.54 4.40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 10000: 83.3 [2.25 -11.78 5.40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 12500: 83.3 [2.28 -13.91 6.90]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 15000: 83.3 [2.82 -16.30 8.00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 17500: 100.0 [3.48 -18.40 9.40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 20000: 83.3 [4.08 -20.44 10.60]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 22500: 100.0 [4.17 -22.63 11.50]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 25000: 100.0 [4.35 -24.37 12.50]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 27500: 100.0 [4.68 -26.08 13.40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 30000: 100.0 [4.92 -27.60 14.20]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 32500: 100.0 [5.04 -27.78 14.40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 35000: 100.0 [5.04 -27.78 14.40]\n"
     ]
    }
   ],
   "source": [
    "a, b, c, m = param(1, -2, -1, 1)  # initial solution\n",
    "x, y, label = const(0, 0, 0)  # not affected by backprop\n",
    "f = minimum(1, label * m * (a * x + b * y + c))\n",
    "k = norm(a, b, c)  # konstraint\n",
    "\n",
    "for iteration in range(60000):\n",
    "\n",
    "    if iteration % 500 == 0 or (iteration % 10 == 0 and iteration < 40):\n",
    "        correct = sum(f.compute() > 0 for (x.val, y.val), label.val in dataset)\n",
    "        print 'Accuracy at iteration {}: {:.1f} [{:.2f} {:.2f} {:.2f} * {:.2f}]'.format(\n",
    "            iteration, (100.0 * correct) / len(dataset), a.val, b.val, c.val, m.val)\n",
    "\n",
    "    (x.val, y.val), label.val = choice(dataset)\n",
    "\n",
    "    f.compute()\n",
    "    k.compute()  # this zeros gradients on a, b, c\n",
    "\n",
    "    f.backprop(grad=1)\n",
    "    k.backprop(grad=0.01 * (1 if k < 1 else -1))\n",
    "\n",
    "    f.update_parameters(lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 0: 66.7 [1.00 -2.00 -1.00]\nAccuracy at iteration 10: 83.3 [0.28 -2.13 -0.40]\nAccuracy at iteration 20: 83.3 [0.22 -2.09 -0.10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 30: 83.3 [0.34 -2.02 -0.00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 2500: 83.3 [0.94 -4.96 1.80]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 5000: 83.3 [1.69 -7.12 3.10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 7500: 83.3 [1.57 -9.47 4.20]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 10000: 83.3 [2.05 -11.61 5.60]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 12500: 83.3 [2.65 -13.61 7.20]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 15000: 100.0 [3.13 -16.00 7.90]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 17500: 100.0 [3.58 -18.24 9.30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 20000: 83.3 [4.12 -20.38 10.30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 22500: 83.3 [4.48 -22.34 11.50]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 25000: 100.0 [4.45 -24.39 12.60]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 27500: 100.0 [4.78 -26.19 13.60]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 30000: 100.0 [5.11 -27.80 14.30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 32500: 100.0 [5.11 -28.15 14.60]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 35000: 100.0 [5.11 -28.15 14.60]\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "from utils.sugar import *\n",
    "\n",
    "dataset = (((1.2, 0.7), +1.0), ((-0.3, 0.5), -1.0), ((-3.0, -1.0), +1.0),\n",
    "           ((0.1, 1.0), -1.0), ((3.0, 1.1), -1.0), ((2.1, -3.0), +1.0))\n",
    "\n",
    "a, b, c = param(1, -2, -1)  # initial solution\n",
    "x, y, label = const(0, 0, 0)  # not affected by backprop\n",
    "f = minimum(1, label * (a * x + b * y + c))\n",
    "\n",
    "for iteration in range(35001):\n",
    "\n",
    "    if iteration % 2500 == 0 or (iteration % 10 == 0 and iteration < 40):\n",
    "        correct = sum(f.compute() > 0 for (x.val, y.val), label.val in dataset)\n",
    "        print 'Accuracy at iteration {}: {:.1f} [{:.2f} {:.2f} {:.2f}]'.format(\n",
    "            iteration, (100.0 * correct) / len(dataset), a.val, b.val, c.val)\n",
    "\n",
    "    (x.val, y.val), label.val = choice(dataset)\n",
    "    f.compute()\n",
    "    f.backprop()\n",
    "    \n",
    "    # a.grad += -a.val\n",
    "    # b.grad += -b.val\n",
    "\n",
    "    f.update_parameters(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Javascript code\n",
    "```javascript\n",
    "var n1 = Math.max(0, a1*x + b1*y + c1); // activation of 1st hidden neuron\n",
    "var n2 = Math.max(0, a2*x + b2*y + c2); // 2nd neuron\n",
    "var n3 = Math.max(0, a3*x + b3*y + c3); // 3rd neuron\n",
    "var score = a4*n1 + b4*n2 + c4*n3 + d4;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "from utils.sugar import *\n",
    "\n",
    "dataset = (((1.2, 0.7), +1.0), ((-0.3, 0.5), -1.0), ((-3.0, -1.0), +1.0),\n",
    "           ((0.1, 1.0), -1.0), ((3.0, 1.1), -1.0), ((2.1, -3.0), +1.0))\n",
    "\n",
    "\n",
    "def hn(xx, yy):  # hidden neuron relu(ax + by + c)\n",
    "    return relu(param() * xx + param() * yy + param())\n",
    "\n",
    "\n",
    "x, y = const(0, 0)  # score = w_0 + sum_1^3 w_i * hn_i(x, y)\n",
    "score = summation([param()] + [param() * hn(x, y) for _ in range(3)])\n",
    "\n",
    "for par in score.parameters():  # randomly initialize weights\n",
    "    par.val = random() - 0.5\n",
    "\n",
    "for iteration in range(35001):\n",
    "\n",
    "    if iteration % 2500 == 0 or (iteration % 10 == 0 and iteration < 40):\n",
    "        correct = sum(score.compute() > 0 for (x.val, y.val), label.val in dataset)\n",
    "        print 'Accuracy at iteration {}: {:.1f} [{:.2f} {:.2f} {:.2f}]'.format(\n",
    "            iteration, (100.0 * correct) / len(dataset), a.val, b.val, c.val)\n",
    "\n",
    "    (x.val, y.val), label.val = choice(dataset)\n",
    "    score.compute()\n",
    "    score.backprop(0.01)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
