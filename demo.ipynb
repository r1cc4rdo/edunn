{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Educational neural networks in Python\n",
    "\n",
    "This code is loosely inspired to [Andrej Karpathy](https://cs.stanford.edu/people/karpathy/)'s excellent but discontinued [Hacker's guide to Neural Networks](http://karpathy.github.io/neuralnets/).\n",
    "\n",
    "This implementation is not a one-to-one transliteration of the original javascript code into Python, but [there](https://github.com/urwithajit9/HG_NeuralNetwork) [are](https://github.com/johnashu/hackers_guide_to_neural_networks) [many](https://github.com/saiashirwad/Hackers-Guide-To-Neural-Networks-Python) [repositories](https://github.com/pannous/karpathy_neuralnets_python) [on](https://github.com/techniquark/Hacker-s-Guide-to-Neural-Networks-in-Python) [Github](https://github.com/Mutinix/hacker-nn/) that closely match it line-by-line. Use those to follow along the blog post.\n",
    "\n",
    "The purpose of this version is to simplify network definition and automate the computation of forward and backward passes. Both these tasks are exploded and manual (for clarity's sake!) in Karpathy's code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base case: single gate in the circuit\n",
    "\n",
    "Shows a single gate implementing f(x,y) = xy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a * b =  -3.0\n"
     ]
    }
   ],
   "source": [
    "from gates import *\n",
    "\n",
    "a = Constant(3)\n",
    "b = Constant(-1)\n",
    "ab = MulGate(a, b)\n",
    "\n",
    "print 'a * b = ', ab.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a * b =  -3.0\n"
     ]
    }
   ],
   "source": [
    "from utils.sugar import *\n",
    "\n",
    "a = const(3)\n",
    "b = const(-1)\n",
    "ab = a * b\n",
    "\n",
    "print 'a * b = ', ab.compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy #1: Random Local Search\n",
    "\n",
    "Random search, perturb inputs and accept them if they improve the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial output: -2 * 3 = -6\nFinal output: -1.8 * 2.93 = -5.28\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "\n",
    "best_in = (-2, 3)\n",
    "best_out = best_in[0] * best_in[1]\n",
    "\n",
    "print 'Initial output: {} * {} = {}'.format(best_in[0], best_in[1], best_out)\n",
    "\n",
    "tweak_amount = 0.01\n",
    "for _ in range(100):\n",
    "    \n",
    "    best_plus_noise = tuple(x + tweak_amount * (random() * 2 - 1) for x in best_in)\n",
    "    out = best_plus_noise[0] * best_plus_noise[1]\n",
    "    if out > best_out:\n",
    "        best_in = best_plus_noise\n",
    "        best_out = out\n",
    "        \n",
    "print 'Final output: {:.3} * {:.3} = {:.3}'.format(best_in[0], best_in[1], best_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy #2: Numerical Gradient\n",
    "\n",
    "Perform one step of numerical gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial output: -6\nFinal output: -5.87\n"
     ]
    }
   ],
   "source": [
    "a, b = -2, 3  # initial inputs\n",
    "eps = 0.0001  # tweak amount\n",
    "out = a * b\n",
    "\n",
    "da = ((a + eps) * b - out) / eps  # 3.0\n",
    "db = (a * (b + eps) - out) / eps  # -2.0\n",
    "\n",
    "step_size = 0.01\n",
    "a, b = a + step_size * da, b + step_size * db\n",
    "print 'Initial output: {}\\nFinal output: {:.3}'.format(out, a * b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy #3: Analytical Gradient\n",
    "\n",
    "Perform one step of gradient descent using analytical derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = param(-2)\n",
    "b = param(3)\n",
    "ab = a * b\n",
    "\n",
    "print 'Initial output: {:}'.format(ab.compute())\n",
    "ab.backprop(lr=0.01)\n",
    "print 'Final output: {:.3}'.format(ab.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Case: Circuits with Multiple Gates\n",
    "\n",
    "Here's an example with multiple gates that depend on each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial output: -12.0\nFinal output: -11.59\n"
     ]
    }
   ],
   "source": [
    "x, y, z = param(-2, 5, -4)\n",
    "xpyz = (x + y) * z\n",
    "\n",
    "print 'Initial output: {:}'.format(xpyz.compute())  # -12\n",
    "xpyz.backprop(0.01)\n",
    "print 'Final output: {:.4}'.format(xpyz.compute())  # -11.59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare analytical and numerical gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param value: -2.04, Analytical grad: -1.6674, Numerical grad: -1.6674, Abs. Diff: 1.2975e-11\nParam value: 4.96, Analytical grad: -1.6674, Numerical grad: -1.6674, Abs. Diff: -6.1632e-11\nParam value: -3.97, Analytical grad: 1.2264, Numerical grad: 1.2264, Abs. Diff: -7.2884e-12\n"
     ]
    }
   ],
   "source": [
    "from tests.numerical_gradients import check_gradients\n",
    "assert(check_gradients(xpyz, verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: single neuron\n",
    "\n",
    "A 2-dimensional neuron computes the following function f(x,y,a,b,c) = σ(ax + by + c) where σ is the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param value: 1.0, Analytical grad: -0.044098, Numerical grad: -0.044097, Abs. Diff: -1.6793e-07\nParam value: 2.0, Analytical grad: 0.13229, Numerical grad: 0.13229, Abs. Diff: -1.5113e-06\nParam value: -3.0, Analytical grad: 0.044097, Numerical grad: 0.044097, Abs. Diff: -1.6792e-07\n---\nInitial output: 0.880797077978\nFinal output: 0.882004356849\n"
     ]
    }
   ],
   "source": [
    "a, b, c = param(1.0, 2.0, -3.0)\n",
    "x, y = const(-1.0, 3.0)\n",
    "s = sigmoid(a * x + b * y + c)\n",
    "\n",
    "assert(check_gradients(s, verbose=True))\n",
    "print '---'\n",
    "print 'Initial output: {}'.format(s.compute())  # 0.880797077978\n",
    "s.backprop(lr=0.01)\n",
    "print 'Final output: {}'.format(s.compute())  # 0.882\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single neuron can also be defined as a single gate with five inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param value: 1.0, Analytical grad: 0.060897, Numerical grad: 0.060897, Abs. Diff: 2.319e-07\nParam value: 2.0, Analytical grad: -0.18268, Numerical grad: -0.18269, Abs. Diff: 2.087e-06\nParam value: -3.0, Analytical grad: -0.060896, Numerical grad: -0.060896, Abs. Diff: 2.319e-07\n---\nInitial output: 0.880797077978\nFinal output: 0.882004356849\n"
     ]
    }
   ],
   "source": [
    "a, b, c = param(1.0, 2.0, -3.0)\n",
    "x, y = const(-1.0, 3.0)\n",
    "n = neuron(a, x, b, y, c)\n",
    "\n",
    "assert(check_gradients(n, verbose=True))\n",
    "print '---'\n",
    "print 'Initial output: {}'.format(n.compute())  # 0.880797077978\n",
    "n.backprop(lr=0.01)\n",
    "print 'Final output: {}'.format(n.compute())  # 0.882\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 0: 66.7 [1.00 -2.00 -1.00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 10: 83.3 [0.10 -2.30 -0.70]\nAccuracy at iteration 20: 83.3 [0.22 -2.23 -0.60]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nAccuracy at iteration 30: 83.3 [0.04 -2.26 -0.40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 2500: 83.3 [1.05 -4.88 1.70]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 5000: 83.3 [1.47 -7.03 3.10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 7500: 83.3 [1.83 -9.46 4.20]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 10000: 83.3 [2.16 -11.78 5.60]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 12500: 100.0 [2.67 -13.88 6.80]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 15000: 100.0 [3.09 -15.96 8.00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 17500: 83.3 [3.09 -18.21 8.90]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 20000: 100.0 [3.42 -20.36 10.20]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 22500: 100.0 [4.17 -22.14 11.40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 25000: 100.0 [4.11 -24.18 12.20]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 27500: 100.0 [4.89 -25.70 13.40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 30000: 100.0 [5.07 -27.34 14.20]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 32500: 100.0 [5.04 -27.78 14.40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 35000: 100.0 [5.04 -27.78 14.40]\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "from utils.sugar import *\n",
    "\n",
    "dataset = (((1.2, 0.7), +1.0), ((-0.3, 0.5), -1.0), ((-3.0, -1.0), +1.0),\n",
    "           ((0.1, 1.0), -1.0), ((3.0, 1.1), -1.0), ((2.1, -3.0), +1.0))\n",
    "\n",
    "a, b, c = param(1, -2, -1)  # initial solution\n",
    "x, y, label = const(0, 0, 0)  # not affected by backprop\n",
    "f = minimum(1, label * (a * x + b * y + c))\n",
    "\n",
    "for iteration in range(35001):\n",
    "\n",
    "    if iteration % 2500 == 0 or (iteration % 10 == 0 and iteration < 40):\n",
    "        correct = sum(f.compute() > 0 for (x.val, y.val), label.val in dataset)\n",
    "        print 'Accuracy at iteration {}: {:.1f} [{:.2f} {:.2f} {:.2f}]'.format(\n",
    "            iteration, (100.0 * correct) / len(dataset), a.val, b.val, c.val)\n",
    "\n",
    "    (x.val, y.val), label.val = choice(dataset)\n",
    "    f.compute()\n",
    "    f.backprop()\n",
    "    \n",
    "    # a.grad += -a.val\n",
    "    # b.grad += -b.val\n",
    "\n",
    "    f.update_parameters(0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
