{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Educational neural networks in Python\n",
    "\n",
    "This code is loosely inspired to [Andrej Karpathy](https://cs.stanford.edu/people/karpathy/)'s excellent [Hacker's guide to Neural Networks](http://karpathy.github.io/neuralnets/).\n",
    "\n",
    "This implementation is not a one-to-one transliteration of the original javascript code into Python, but [there](https://github.com/urwithajit9/HG_NeuralNetwork) [are](https://github.com/johnashu/hackers_guide_to_neural_networks) [many](https://github.com/saiashirwad/Hackers-Guide-To-Neural-Networks-Python) [repositories](https://github.com/pannous/karpathy_neuralnets_python) [on](https://github.com/techniquark/Hacker-s-Guide-to-Neural-Networks-in-Python) [Github](https://github.com/Mutinix/hacker-nn/) that closely match it line-by-line. Use those to follow along the blog post.\n",
    "\n",
    "The main purpose of this version is to simplify network definition and automate the computation of forward and backward passes. Both these tasks are exploded and manual (for clarity's sake!) in Karpathy's code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base case: single gate in the circuit\n",
    "\n",
    "Shows a single gate implementing f(x,y) = xy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a * b =  -3\n"
     ]
    }
   ],
   "source": [
    "from iogates import Constant\n",
    "from opgates import MulGate\n",
    "\n",
    "a = Constant(3)\n",
    "b = Constant(-1)\n",
    "ab = MulGate(a, b)\n",
    "\n",
    "print 'a * b = ', ab.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a * b =  -3\n"
     ]
    }
   ],
   "source": [
    "from sugar import *\n",
    "\n",
    "a = const(3)\n",
    "b = const(-1)\n",
    "ab = a * b\n",
    "\n",
    "print 'a * b = ', ab.compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy #1: Random Local Search\n",
    "\n",
    "Random search, perturb inputs and accept them if they improve the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial output: -2 * 3 = -6\nFinal output: -1.82 * 2.83 = -5.17\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "\n",
    "best_in = (-2, 3)\n",
    "best_out = best_in[0] * best_in[1]\n",
    "\n",
    "print 'Initial output: {} * {} = {}'.format(best_in[0], best_in[1], best_out)\n",
    "\n",
    "tweak_amount = 0.01\n",
    "for _ in range(100):\n",
    "    \n",
    "    best_plus_noise = tuple(x + tweak_amount * (random() * 2 - 1) for x in best_in)\n",
    "    out = best_plus_noise[0] * best_plus_noise[1]\n",
    "    if out > best_out:\n",
    "        best_in = best_plus_noise\n",
    "        best_out = out\n",
    "        \n",
    "print 'Final output: {:.3} * {:.3} = {:.3}'.format(best_in[0], best_in[1], best_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy #2: Numerical Gradient\n",
    "\n",
    "Perform one step of numerical gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial output: -6\nFinal output: -5.87\n"
     ]
    }
   ],
   "source": [
    "a, b = -2, 3  # initial inputs\n",
    "eps = 0.0001  # tweak amount\n",
    "out = a * b\n",
    "\n",
    "da = ((a + eps) * b - out) / eps  # 3.0\n",
    "db = (a * (b + eps) - out) / eps  # -2.0\n",
    "\n",
    "step_size = 0.01\n",
    "a, b = a + step_size * da, b + step_size * db\n",
    "print 'Initial output: {}\\nFinal output: {:.3}'.format(out, a * b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy #3: Analytic Gradient\n",
    "\n",
    "Perform one step of gradient descent using analytical derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial output: -6\nFinal output: -5.87\n"
     ]
    }
   ],
   "source": [
    "a = param(-2)\n",
    "b = param(3)\n",
    "ab = a * b\n",
    "\n",
    "print 'Initial output: {:}'.format(ab.compute())\n",
    "ab.backprop(lr=0.01)\n",
    "print 'Final output: {:.3}'.format(ab.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Case: Circuits with Multiple Gates\n",
    "\n",
    "Here's an example with multiple gates that depend on each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial output: -12\nFinal output: -11.59\n"
     ]
    }
   ],
   "source": [
    "x, y, z = param((-2, 5, -4))\n",
    "xpyz = (x + y) * z\n",
    "\n",
    "print 'Initial output: {:}'.format(xpyz.compute())  # -12\n",
    "xpyz.backprop(0.01)\n",
    "print 'Final output: {:.4}'.format(xpyz.compute())  # -11.59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare analytical and numerical gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param value: -2.04, Analytical grad: -3.97, Numerical grad: -3.97, Diff: 1.2167e-06\nParam value: 4.96, Analytical grad: -3.97, Numerical grad: -3.97, Diff: 1.2167e-06\nParam value: -3.97, Analytical grad: 2.92, Numerical grad: 2.92, Diff: -2.416e-07\n"
     ]
    }
   ],
   "source": [
    "from utils import check_gradients\n",
    "assert(check_gradients(xpyz, verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: single neuron\n",
    "\n",
    "A 2-dimensional neuron tcomputes the following function f(x,y,a,b,c) = σ(ax + by + c) where σ is the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param value: 1.0, Analytical grad: -0.10499, Numerical grad: -0.10499, Diff: -2.3805e-07\nParam value: 2.0, Analytical grad: 0.31498, Numerical grad: 0.31498, Diff: -6.2994e-08\nParam value: -3.0, Analytical grad: 0.10499, Numerical grad: 0.10499, Diff: -9.5013e-08\n---\nInitial output: 0.880797077978\nFinal output: 0.882\n"
     ]
    }
   ],
   "source": [
    "a, b, c = param((1.0, 2.0, -3.0))\n",
    "x, y = const((-1.0, 3.0))\n",
    "s = sigmoid(a * x + b * y + c)\n",
    "\n",
    "assert(check_gradients(s, verbose=True))\n",
    "print '---'\n",
    "print 'Initial output: {:}'.format(s.compute())  # 0.880797077978\n",
    "s.backprop(0.01)\n",
    "print 'Final output: {:.5}'.format(s.compute())  # 0.882\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single neuron can also be defined as a single gate with five inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param value: 1.0, Analytical grad: -0.10499, Numerical grad: -0.10499, Diff: -2.3805e-07\nParam value: 2.0, Analytical grad: 0.31498, Numerical grad: 0.31498, Diff: -6.2994e-08\nParam value: -3.0, Analytical grad: 0.10499, Numerical grad: 0.10499, Diff: -9.5013e-08\n---\nInitial output: 0.880797077978\nFinal output: 0.882\n"
     ]
    }
   ],
   "source": [
    "a, b, c = param((1.0, 2.0, -3.0))\n",
    "x, y = const((-1.0, 3.0))\n",
    "n = neuron(a, b, c, x, y)\n",
    "\n",
    "assert(check_gradients(n, verbose=True))\n",
    "print '---'\n",
    "print 'Initial output: {:}'.format(n.compute())  # 0.880797077978\n",
    "n.backprop(0.01)\n",
    "print 'Final output: {:.5}'.format(n.compute())  # 0.882\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at iteration 24: 0\nAccuracy at iteration 49: 0\nAccuracy at iteration 74: 0\nAccuracy at iteration 99: 0\nAccuracy at iteration 124: 0\nAccuracy at iteration 149: 0\nAccuracy at iteration 174: 0\nAccuracy at iteration 199: 0\nAccuracy at iteration 224: 0\nAccuracy at iteration 249: 0\nAccuracy at iteration 274: 0\nAccuracy at iteration 299: 0\nAccuracy at iteration 324: 0\nAccuracy at iteration 349: 0\nAccuracy at iteration 374: 0\nAccuracy at iteration 399: 0\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "\n",
    "dataset = (((1.2, 0.7), +1), ((-0.3, 0.5), -1), ((-3, -1), +1),\n",
    "           ((0.1, 1.0), -1), ((3.0, 1.1), -1), ((2.1, -3), +1))\n",
    "\n",
    "a, b, c = param((1.0, 2.0, -3.0))\n",
    "x, y = const(dataset[0][0])  # constant in the sense that it is not updated by incoming gradients\n",
    "f = a * x + b * y + c\n",
    "\n",
    "\n",
    "def evaluate_training_accuracy():\n",
    "    total_correct = 0\n",
    "    for training_example in dataset:\n",
    "        (x.val, y.val), label = training_example\n",
    "        output = 1 if f.compute() > 0 else -1\n",
    "        correct = 1 if output == label else 0\n",
    "        total_correct += correct\n",
    "    return total_correct / len(dataset)\n",
    "\n",
    "\n",
    "for iteration in range(400):\n",
    "    \n",
    "    (x.val, y.val), label = choice(dataset)\n",
    "    output = f.compute()\n",
    "    \n",
    "    f.grad = 0\n",
    "    if label == 1 and output < 1:\n",
    "        f.grad = 1\n",
    "    if label == -1 and output > -1:\n",
    "        f.grad = -1\n",
    "    f.backprop()\n",
    "\n",
    "    for p in f.parameters():\n",
    "        p.grad += p.val\n",
    "    f.update_parameters(0.01)\n",
    "\n",
    "    if (iteration + 1) % 25 == 0:\n",
    "        print 'Accuracy at iteration {}: {}'.format(iteration, evaluate_training_accuracy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
