{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Educational neural networks in Python\n",
    "\n",
    "This code is loosely inspired to [Andrej Karpathy](https://cs.stanford.edu/people/karpathy/)'s excellent [Hacker's guide to Neural Networks](http://karpathy.github.io/neuralnets/).\n",
    "\n",
    "This implementation is not a one-to-one transliteration of the original javascript code into Python, but [there](https://github.com/urwithajit9/HG_NeuralNetwork) [are](https://github.com/johnashu/hackers_guide_to_neural_networks) [many](https://github.com/saiashirwad/Hackers-Guide-To-Neural-Networks-Python) [repositories](https://github.com/pannous/karpathy_neuralnets_python) [on](https://github.com/techniquark/Hacker-s-Guide-to-Neural-Networks-in-Python) [Github](https://github.com/Mutinix/hacker-nn/) that closely match it line-by-line. Use those to follow along the blog post.\n",
    "\n",
    "The main purpose of this version is to simplify network definition and automate the computation of forward and backward passes. Both these tasks are exploded and manual (for clarity's sake!) in Karpathy's code.\n",
    "\n",
    "For example, a single neuron can be written as:\n",
    "\n",
    "```python\n",
    "a, b, c = param([1.0, 2.0, -3.0])\n",
    "x, y = const([-1.0, 3.0])\n",
    "\n",
    "s = sigmoid(a * x + b * y + c)\n",
    "print s.compute()  # 0.880797077978\n",
    "```\n",
    "\n",
    "Compare it with the original implementation:\n",
    "\n",
    "```javascript\n",
    "var a = new Unit(1.0, 0.0);\n",
    "var b = new Unit(2.0, 0.0);\n",
    "var c = new Unit(-3.0, 0.0);\n",
    "var x = new Unit(-1.0, 0.0);\n",
    "var y = new Unit(3.0, 0.0);\n",
    "\n",
    "// create the gates\n",
    "var mulg0 = new multiplyGate();\n",
    "var mulg1 = new multiplyGate();\n",
    "var addg0 = new addGate();\n",
    "var addg1 = new addGate();\n",
    "var sg0 = new sigmoidGate();\n",
    "\n",
    "// do the forward pass\n",
    "var forwardNeuron = function() {\n",
    "  ax = mulg0.forward(a, x); // a*x = -1\n",
    "  by = mulg1.forward(b, y); // b*y = 6\n",
    "  axpby = addg0.forward(ax, by); // a*x + b*y = 5\n",
    "  axpbypc = addg1.forward(axpby, c); // a*x + b*y + c = 2\n",
    "  s = sg0.forward(axpbypc); // sig(a*x + b*y + c) = 0.8808\n",
    "};\n",
    "forwardNeuron();\n",
    "\n",
    "console.log('circuit output: ' + s.value); // prints 0.8808\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base case: single gate in the circuit\n",
    "\n",
    "Shows a single gate implementing f(x,y) = xy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a * b =  -3\n"
     ]
    }
   ],
   "source": [
    "from iogates import Constant\n",
    "from opgates import MulGate\n",
    "\n",
    "a = Constant(3)\n",
    "b = Constant(-1)\n",
    "ab = MulGate(a, b)\n",
    "\n",
    "print 'a * b = ', ab.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* a gate has `forward()` and `backward()` methods that respectively evaluate the output of a gate given its inputs, or propagate the gradients from the output to inputs.\n",
    "* A gate graph is assumed to be a directed and cycle-free. This does not prevent any two nodes to be connected by multiple, distinct paths.\n",
    "* The `compute()` methods recursively and automatically updates a gate and all its ancestors. In the minimal example above, this is equivalent to calling `ab.forward()`\n",
    "\n",
    "With minimal syntactic sugar, the code above can be rewritten as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a * b =  -3\n"
     ]
    }
   ],
   "source": [
    "from sugar import *\n",
    "\n",
    "a = const(3)\n",
    "b = const(-1)\n",
    "ab = a * b\n",
    "\n",
    "print 'a * b = ', ab.compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy #1: Random Local Search\n",
    "\n",
    "Random search, perturb inputs and accept them if they improve the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial output: -2 * 3 = -6\n",
      "Final output: -1.98 * 2.99 = -5.92\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "\n",
    "best_in = (a.val, b.val) = (-2, 3)\n",
    "best_out = ab.compute()\n",
    "\n",
    "print 'Initial output: {} * {} = {}'.format(best_in[0], best_in[1], best_out)\n",
    "\n",
    "tweak_amount = 0.01\n",
    "for _ in range(10):\n",
    "    \n",
    "    a.val, b.val = (x + tweak_amount * (random() * 2 - 1) for x in best_in)\n",
    "    out = ab.compute()\n",
    "    if out > best_out:\n",
    "        best_in = (a.val, b.val)\n",
    "        best_out = out\n",
    "        \n",
    "print 'Final output: {:.3} * {:.3} = {:.3}'.format(best_in[0], best_in[1], best_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy #2: Numerical Gradient\n",
    "\n",
    "Perform one step of numerical gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial output: -6\n",
      "Final output: -5.87\n"
     ]
    }
   ],
   "source": [
    "a.val, b.val = -2, 3  # initial inputs\n",
    "eps = 0.0001  # tweak amount / epsilon\n",
    "out = ab.compute()\n",
    "\n",
    "prev_a, a.val = a.val, a.val + eps\n",
    "dx = (ab.compute() - out) / eps  # 3.0\n",
    "\n",
    "a.val, b.val = prev_a, b.val + eps\n",
    "dy = (ab.compute() - out) / eps  # -2.0\n",
    "\n",
    "step_size = 0.01\n",
    "a.val, b.val = (val + step_size * der for val, der in ((a.val, dx), (b.val, dy)))\n",
    "new_out = ab.compute()  # -5.870797\n",
    "\n",
    "print 'Initial output: {}\\nFinal output: {:.3}'.format(out, new_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy #3: Analytic Gradient\n",
    "\n",
    "Perform one step of gradient descent using analytical derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial output: -6\n",
      "Final output: -5.87\n"
     ]
    }
   ],
   "source": [
    "step_size = 0.01\n",
    "a = param(-2, step_size)\n",
    "b = param(3, step_size)\n",
    "ab = a * b\n",
    "\n",
    "print 'Initial output: {:}'.format(ab.compute())\n",
    "ab.backprop()\n",
    "print 'Final output: {:.3}'.format(ab.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* `const` (constant) gates ignore incoming gradients and do not change their value.\n",
    "* `param` (parameter) gates update the gate value by `step_size * gradient`.\n",
    "* `compute()` (forward) and `backprop()` (backward) passes need to be interleaved.\n",
    "\n",
    "   Calling `backprop()` multiple times in a row performs multiple gradient descent steps in the gradient direction at the initial, unchanging output value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Case: Circuits with Multiple Gates\n",
    "\n",
    "Here's an example with multiple gates that depend on each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial output: -12\n",
      "Final output: -11.59\n"
     ]
    }
   ],
   "source": [
    "x, y, z = param((-2, 5, -4), 0.01)\n",
    "xpyz = (x + y) * z\n",
    "\n",
    "print 'Initial output: {:}'.format(xpyz.compute())  # -12\n",
    "xpyz.backprop()\n",
    "print 'Final output: {:.4}'.format(xpyz.compute())  # -11.59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare analytical and numerical gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param value: -2.04, Analytical grad: -3.97, Numerical grad: -3.97, Diff: 1.2167e-06\n",
      "Param value: 4.96, Analytical grad: -3.97, Numerical grad: -3.97, Diff: 1.2167e-06\n",
      "Param value: -3.97, Analytical grad: 2.92, Numerical grad: 2.92, Diff: -2.416e-07\n"
     ]
    }
   ],
   "source": [
    "assert(xpyz.check_numerical_gradient(verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: single neuron\n",
    "\n",
    "A 2-dimensional neuron tcomputes the following function f(x,y,a,b,c) = σ(ax + by + c) where σ is the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param value: 1.0, Analytical grad: -0.10499, Numerical grad: -0.10499, Diff: -2.3805e-07\n",
      "Param value: 2.0, Analytical grad: 0.31498, Numerical grad: 0.31498, Diff: -6.2994e-08\n",
      "Param value: -3.0, Analytical grad: 0.10499, Numerical grad: 0.10499, Diff: -9.5013e-08\n",
      "---\n",
      "Initial output: 0.880797077978\n",
      "Final output: 0.882\n"
     ]
    }
   ],
   "source": [
    "a, b, c = param([1.0, 2.0, -3.0], lr=0.01)\n",
    "x, y = const([-1.0, 3.0])\n",
    "s = sigmoid(a * x + b * y + c)\n",
    "\n",
    "assert(s.check_numerical_gradient(verbose=True))\n",
    "print '---'\n",
    "print 'Initial output: {:}'.format(s.compute())  # 0.880797077978\n",
    "s.backprop()\n",
    "print 'Final output: {:.5}'.format(s.compute())  # 0.882\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single neuron can also be defined as a single gate with five inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param value: 1.0, Analytical grad: -0.10499, Numerical grad: -0.10499, Diff: -2.3805e-07\n",
      "Param value: 2.0, Analytical grad: 0.31498, Numerical grad: 0.31498, Diff: -6.2994e-08\n",
      "Param value: -3.0, Analytical grad: 0.10499, Numerical grad: 0.10499, Diff: -9.5013e-08\n",
      "---\n",
      "Initial output: 0.880797077978\n",
      "Final output: 0.882\n"
     ]
    }
   ],
   "source": [
    "a, b, c = param([1.0, 2.0, -3.0], lr=0.01)\n",
    "x, y = const([-1.0, 3.0])\n",
    "n = neuron(a, b, c, x, y)\n",
    "\n",
    "assert(n.check_numerical_gradient(verbose=True))\n",
    "print '---'\n",
    "print 'Initial output: {:}'.format(n.compute())  # 0.880797077978\n",
    "n.backprop()\n",
    "print 'Final output: {:.5}'.format(n.compute())  # 0.882\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors = [[1.2, 0.7], [-0.3, 0.5], [-3, -1],\n",
    "           [0.1, 1.0], [3.0, 1.1], [2.1, -3]]\n",
    "labels = [+1, -1, +1, -1, -1, +1]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
